import json
import re
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from collections import defaultdict
from gpt_retrieval.call_gpt import *

OPENAI_API_KEY = ""


class SingleModalEvaluator:
    """å•æ¨¡æ€å›¾åƒè´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, prompt_path, gen_base_path, output_dir, modal, max_workers=8, batch_size=20):
        self.prompt_path = prompt_path
        self.gen_base_path = gen_base_path
        self.output_dir = output_dir
        self.modal = modal  # æŒ‡å®šçš„å•ä¸ªæ¨¡æ€ï¼š'mm', 'cot', 'dir', 'txt', 'img'
        self.max_workers = max_workers
        self.batch_size = batch_size
        
        # çº¿ç¨‹å®‰å…¨çš„ç»“æœå­˜å‚¨
        self.detailed_results = []
        self.summary_results = []
        self.results_lock = Lock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'start_time': None,
            'missing_images': 0
        }
        self.stats_lock = Lock()
        
        # é¢„åŠ è½½æ•°æ®
        self.prompt_data = {}
        self.grouped_questions_cache = {}
        
    def get_image_path(self, prompt_id, modal):
        """è·å–å›¾åƒè·¯å¾„ï¼Œæ”¯æŒå¤§å°å†™å˜ä½“"""
        base_path = f"{self.gen_base_path}/{prompt_id}_{modal}.png"
        
        # é¦–å…ˆå°è¯•åŸå§‹è·¯å¾„
        if os.path.exists(base_path):
            return base_path
            
        # å°è¯•é¦–å­—æ¯å¤§å†™çš„modal
        capitalized_modal = modal.capitalize()
        capitalized_path = f"{self.gen_base_path}/{prompt_id}_{capitalized_modal}.png"
        if os.path.exists(capitalized_path):
            return capitalized_path
            
        # å°è¯•å…¨å¤§å†™çš„modal
        upper_modal = modal.upper()
        upper_path = f"{self.gen_base_path}/{prompt_id}_{upper_modal}.png"
        if os.path.exists(upper_path):
            return upper_path
            
        # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œè¿”å›åŸå§‹è·¯å¾„ï¼ˆç”¨äºé”™è¯¯å¤„ç†ï¼‰
        return base_path
        
    def load_data_once(self):
        print(f"ğŸ“š é¢„åŠ è½½æ•°æ® (æ¨¡æ€: {self.modal})...")
        
        # åŠ è½½æç¤ºæ•°æ®
        with open(self.prompt_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    data = json.loads(line)
                    self.prompt_data[data.get("id")] = data
        
        # é¢„åŠ è½½æ‰€æœ‰ç±»åˆ«çš„é—®é¢˜æ•°æ®
        categories = set(data.get("category") for data in self.prompt_data.values())
        for category in categories:
            if category:
                question_file_path = f""
                if os.path.exists(question_file_path):
                    with open(question_file_path, "r", encoding="utf-8") as f:
                        self.grouped_questions_cache[category] = json.load(f)
                        
        print(f"âœ… å·²åŠ è½½ {len(self.prompt_data)} ä¸ªæç¤ºå’Œ {len(self.grouped_questions_cache)} ä¸ªç±»åˆ«çš„é—®é¢˜æ•°æ®")
    
    def check_images_exist(self):
        """æ£€æŸ¥æŒ‡å®šæ¨¡æ€çš„å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        print(f"ğŸ” æ£€æŸ¥ {self.modal} æ¨¡æ€çš„å›¾åƒæ–‡ä»¶...")
        missing_count = 0
        found_variants = {"original": 0, "capitalized": 0, "upper": 0}
        
        for prompt_id in self.prompt_data.keys():
            img_path = self.get_image_path(prompt_id, self.modal)
            
            # æ£€æŸ¥å®é™…æ‰¾åˆ°çš„æ˜¯å“ªç§å˜ä½“
            if os.path.exists(img_path):
                if f"_{self.modal}.png" in img_path:
                    found_variants["original"] += 1
                elif f"_{self.modal.capitalize()}.png" in img_path:
                    found_variants["capitalized"] += 1
                elif f"_{self.modal.upper()}.png" in img_path:
                    found_variants["upper"] += 1
            else:
                print(f"âš ï¸  {prompt_id}_{self.modal}.png not found (å°è¯•äº†æ‰€æœ‰å¤§å°å†™å˜ä½“)")
                missing_count += 1
        
        with self.stats_lock:
            self.stats['missing_images'] = missing_count
            
        if missing_count > 0:
            print(f"âš ï¸  å‘ç° {missing_count} ä¸ªç¼ºå¤±çš„ {self.modal} æ¨¡æ€å›¾åƒæ–‡ä»¶")
        else:
            print(f"âœ… æ‰€æœ‰ {self.modal} æ¨¡æ€å›¾åƒæ–‡ä»¶éƒ½å­˜åœ¨")
            
        # æ˜¾ç¤ºæ‰¾åˆ°çš„æ–‡ä»¶åæ ¼å¼ç»Ÿè®¡
        if any(found_variants.values()):
            print(f"ğŸ“Š æ–‡ä»¶åæ ¼å¼ç»Ÿè®¡:")
            if found_variants["original"] > 0:
                print(f"   - åŸå§‹æ ¼å¼ ({self.modal}): {found_variants['original']} ä¸ª")
            if found_variants["capitalized"] > 0:
                print(f"   - é¦–å­—æ¯å¤§å†™ ({self.modal.capitalize()}): {found_variants['capitalized']} ä¸ª")
            if found_variants["upper"] > 0:
                print(f"   - å…¨å¤§å†™ ({self.modal.upper()}): {found_variants['upper']} ä¸ª")
    
    def create_evaluation_tasks(self):
        """åˆ›å»ºè¯„ä¼°ä»»åŠ¡åˆ—è¡¨ï¼ˆä»…é’ˆå¯¹æŒ‡å®šæ¨¡æ€ï¼‰"""
        tasks = []
        
        for prompt_id, prompt_data in self.prompt_data.items():
            class_name = prompt_data.get("category")
            if class_name not in self.grouped_questions_cache:
                print(f"âš ï¸  ç±»åˆ« {class_name} çš„é—®é¢˜æ•°æ®æœªæ‰¾åˆ°ï¼Œè·³è¿‡ {prompt_id}")
                continue
                
            grouped_questions = self.grouped_questions_cache[class_name]
            if prompt_id not in grouped_questions:
                print(f"âš ï¸  {prompt_id} ä¸åœ¨é—®é¢˜æ•°æ®ä¸­ï¼Œè·³è¿‡")
                continue
            
            prompt_questions = grouped_questions[prompt_id]
            questions = prompt_questions['questions']
            
            # ä½¿ç”¨æ–°çš„è·¯å¾„è·å–æ–¹æ³•ï¼Œæ”¯æŒå¤§å°å†™å˜ä½“
            generated_img_path = self.get_image_path(prompt_id, self.modal)
            if not os.path.exists(generated_img_path):
                print(f"âš ï¸  è·³è¿‡ {prompt_id}_{self.modal}.png (æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²å°è¯•æ‰€æœ‰å¤§å°å†™å˜ä½“)")
                continue
            
            # ä¸ºæ¯ä¸ªé—®é¢˜åˆ›å»ºä»»åŠ¡ï¼ˆä»…é’ˆå¯¹æŒ‡å®šæ¨¡æ€ï¼‰
            for question_id, question_info in questions.items():
                task = {
                    'prompt_id': prompt_id,
                    'question_id': question_id,
                    'modal': self.modal,
                    'prompt_data': prompt_data,
                    'question_info': question_info,
                    'txt_reference': prompt_questions['txt_reference'],
                    'generated_img_path': generated_img_path
                }
                tasks.append(task)
        
        with self.stats_lock:
            self.stats['total_tasks'] = len(tasks)
            
        print(f"ğŸ“‹ åˆ›å»ºäº† {len(tasks)} ä¸ª {self.modal} æ¨¡æ€è¯„ä¼°ä»»åŠ¡")
        return tasks
    
    def evaluate_single_task(self, task):
        """è¯„ä¼°å•ä¸ªä»»åŠ¡"""
        try:
            # æå–ä»»åŠ¡ä¿¡æ¯
            prompt_id = task['prompt_id']
            question_id = task['question_id']
            modal = task['modal']
            question_info = task['question_info']
            txt_reference = task['txt_reference']
            generated_img_path = task['generated_img_path']
            
            question_content = question_info['content']
            question_type = question_info['modal']  # "T" æˆ– "I"
            question_ref_content = question_info.get('txt_reference', txt_reference)
            ref_img_path = question_info.get('img_path')
            question_class = question_info.get('class_id')
            
            # å‡†å¤‡è¯„ä¼°å®¢æˆ·ç«¯
            eval_client = MultimodalRetrievalClient(OPENAI_API_KEY, model="gpt-5")
            
            # æ ¹æ®é—®é¢˜ç±»å‹å‡†å¤‡è¾“å…¥
            if question_type == "T":
                sys_prompt = f'You are a strict quality inspector for generated images. Based on the Reference: {question_ref_content}, evaluate whether the generated image meets the quality standards. Please respond with True or False and provide concise, key reasoning for your assessment.'
                img_paths = [generated_img_path]
                
            elif question_type == "I":
                sys_prompt = f'You are a strict quality inspector for generated images. Based on the Reference: {question_ref_content}, compare the reference image <IMAGE_0> with the generated image to evaluate quality. Please respond with True or False and provide concise, key reasoning for your assessment.'
                full_ref_img_path = f""
                img_paths = [full_ref_img_path, generated_img_path]
            else:
                return {
                    'status': 'error',
                    'task': task,
                    'error': f"Unknown question type: {question_type}"
                }
            
            # å‘é€è¯„ä¼°è¯·æ±‚
            evaluation_response = eval_client.send_single_message(
                text=question_content, 
                image_paths=img_paths, 
                system_prompt=sys_prompt
            )
            
            # è§£æè¯„ä¼°ç»“æœ
            if 'True' in evaluation_response:
                score = True
            elif 'False' in evaluation_response:
                score = False
            else:
                score = None
            
            # æ›´æ–°ç»Ÿè®¡
            with self.stats_lock:
                self.stats['completed_tasks'] += 1
                if self.stats['completed_tasks'] % 5 == 0:
                    elapsed = time.time() - self.stats['start_time']
                    rate = self.stats['completed_tasks'] / elapsed
                    remaining = (self.stats['total_tasks'] - self.stats['completed_tasks']) / rate
                    print(f"â³ è¿›åº¦: {self.stats['completed_tasks']}/{self.stats['total_tasks']} "
                          f"({self.stats['completed_tasks']/self.stats['total_tasks']*100:.1f}%) "
                          f"é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ")
            
            return {
                'status': 'success',
                'task': task,
                'evaluation_response': evaluation_response,
                'score': score,
                'question_class': question_class,
                'question_type': question_type,
                'question_content': question_content,
                'txt_reference': question_ref_content,
                'ref_img_path': ref_img_path
            }
            
        except Exception as e:
            with self.stats_lock:
                self.stats['failed_tasks'] += 1
            
            return {
                'status': 'error',
                'task': task,
                'error': str(e)
            }
    
    def process_results(self, results):
        """å¤„ç†å¹¶ç»„ç»‡ç»“æœ"""
        print("ğŸ“Š ç»„ç»‡ç»“æœæ•°æ®...")
        
        # æŒ‰prompt_idåˆ†ç»„ç»“æœ
        prompt_results = defaultdict(lambda: {
            'detailed': {'questions': {}},
            'summary': {'question_evaluations': []}
        })
        
        for result in results:
            if result['status'] != 'success':
                continue
                
            task = result['task']
            prompt_id = task['prompt_id']
            question_id = task['question_id']
            modal = task['modal']
            prompt_data = task['prompt_data']
            
            # åˆå§‹åŒ–promptçº§åˆ«çš„ä¿¡æ¯
            if 'prompt_id' not in prompt_results[prompt_id]['detailed']:
                prompt_results[prompt_id]['detailed'].update({
                    'prompt_id': prompt_id,
                    'prompt_en': prompt_data.get('prompt_en'),
                    'txt_reference': task['txt_reference'],
                    'modal': modal  # æ·»åŠ æ¨¡æ€ä¿¡æ¯
                })
                prompt_results[prompt_id]['summary'].update({
                    'prompt_id': prompt_id,
                    'prompt_en': prompt_data.get('prompt_en'),
                    'modal': modal  # æ·»åŠ æ¨¡æ€ä¿¡æ¯
                })
            
            # åˆå§‹åŒ–questionçº§åˆ«çš„ä¿¡æ¯
            if question_id not in prompt_results[prompt_id]['detailed']['questions']:
                prompt_results[prompt_id]['detailed']['questions'][question_id] = {
                    'question_content': result['question_content'],
                    'question_type': result['question_type'],
                    'question_class': result['question_class'],
                    'txt_reference': result['txt_reference'],
                    'ref_img_path': result['ref_img_path'],
                    'evaluation_response': result['evaluation_response'],
                    'score': result['score']
                }
                
                # åœ¨summaryä¸­æ·»åŠ è¯¥é—®é¢˜
                summary_question = {
                    'question_id': question_id,
                    'question_class': result['question_class'],
                    'question_content': result['question_content'],
                    'question_type': result['question_type'],
                    'score': result['score']
                }
                prompt_results[prompt_id]['summary']['question_evaluations'].append(summary_question)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        detailed_results = [data['detailed'] for data in prompt_results.values()]
        summary_results = [data['summary'] for data in prompt_results.values()]
        
        return detailed_results, summary_results
    
    def save_results(self, detailed_results, summary_results):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_output_path = f"{self.output_dir}/detailed_evaluation_results_{self.modal}_modal.json"
        with open(detailed_output_path, "w", encoding="utf-8") as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‘˜è¦ç»“æœ
        summary_output_path = f"{self.output_dir}/summary_evaluation_results_{self.modal}_modal.json"
        with open(summary_output_path, "w", encoding="utf-8") as f:
            json.dump(summary_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¯¦ç»†ç»“æœä¿å­˜åˆ°: {detailed_output_path}")
        print(f"ğŸ’¾ æ‘˜è¦ç»“æœä¿å­˜åˆ°: {summary_output_path}")
        
        return detailed_output_path, summary_output_path
    
    def run_single_modal_evaluation(self):
        """è¿è¡Œå•æ¨¡æ€è¯„ä¼°"""
        print(f"ğŸš€ å¼€å§‹ {self.modal} æ¨¡æ€å›¾åƒè´¨é‡è¯„ä¼°")
        print("=" * 60)
        
        # é¢„åŠ è½½æ•°æ®
        self.load_data_once()
        
        # æ£€æŸ¥å›¾åƒ
        self.check_images_exist()
        
        # åˆ›å»ºä»»åŠ¡
        tasks = self.create_evaluation_tasks()
        if not tasks:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å¯æ‰§è¡Œçš„ {self.modal} æ¨¡æ€ä»»åŠ¡")
            return None, None
        
        # å¼€å§‹è®¡æ—¶
        with self.stats_lock:
            self.stats['start_time'] = time.time()
        
        print(f"ğŸ”§ ä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†")
        print(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {self.batch_size}")
        print()
        
        all_results = []
        
        # åˆ†æ‰¹å¤„ç†ä»»åŠ¡
        for i in range(0, len(tasks), self.batch_size):
            batch_tasks = tasks[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(tasks) + self.batch_size - 1) // self.batch_size
            
            print(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_tasks)} ä¸ªä»»åŠ¡)")
            
            # å¹¶è¡Œæ‰§è¡Œå½“å‰æ‰¹æ¬¡
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_task = {executor.submit(self.evaluate_single_task, task): task 
                                for task in batch_tasks}
                
                batch_results = []
                for future in as_completed(future_to_task):
                    result = future.result()
                    batch_results.append(result)
                    
                    # å®æ—¶æ˜¾ç¤ºé”™è¯¯
                    if result['status'] == 'error':
                        task = result['task']
                        print(f"âŒ ä»»åŠ¡å¤±è´¥: {task['prompt_id']}_{task['question_id']}_{task['modal']} - {result['error']}")
            
            all_results.extend(batch_results)
            
            # æ‰¹æ¬¡å®Œæˆåçš„ç»Ÿè®¡
            success_count = sum(1 for r in batch_results if r['status'] == 'success')
            error_count = len(batch_results) - success_count
            print(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ: {success_count} æˆåŠŸ, {error_count} å¤±è´¥")
        
        # å¤„ç†æœ€ç»ˆç»“æœ
        print("\nğŸ“Š å¤„ç†æœ€ç»ˆç»“æœ...")
        detailed_results, summary_results = self.process_results(all_results)
        
        # ä¿å­˜ç»“æœ
        self.save_results(detailed_results, summary_results)
        
        # æœ€ç»ˆç»Ÿè®¡
        elapsed_time = time.time() - self.stats['start_time']
        print(f"\nğŸ“ˆ {self.modal} æ¨¡æ€è¯„ä¼°å®Œæˆç»Ÿè®¡:")
        print("=" * 40)
        print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time/60:.2f} åˆ†é’Ÿ")
        print(f"ğŸ“‹ æ€»ä»»åŠ¡æ•°: {self.stats['total_tasks']}")
        print(f"âœ… æˆåŠŸä»»åŠ¡: {self.stats['completed_tasks']}")
        print(f"âŒ å¤±è´¥ä»»åŠ¡: {self.stats['failed_tasks']}")
        print(f"ğŸ–¼ï¸  ç¼ºå¤±å›¾åƒ: {self.stats['missing_images']}")
        print(f"ğŸ“Š å¤„ç†äº† {len(detailed_results)} ä¸ªæç¤º")
        print(f"âš¡ å¹³å‡é€Ÿåº¦: {self.stats['completed_tasks']/(elapsed_time/60):.1f} ä»»åŠ¡/åˆ†é’Ÿ")
        
        # è®¡ç®—æ€»é—®é¢˜æ•°å’Œè¯„ä¼°æ•°
        total_questions = sum(len(item['questions']) for item in detailed_results)
        total_evaluations = len(all_results)
        
        print(f"â“ æ€»é—®é¢˜æ•°: {total_questions}")
        print(f"ğŸ”¬ æ€»è¯„ä¼°æ•°: {total_evaluations}")
        
        return detailed_results, summary_results


def main():
    """ä¸»å‡½æ•°"""
    # ç›´æ¥è®¾ç½®å‚æ•°å€¼ï¼Œä¸ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    prompt_path = ""
    gen_base_path = ""
    output_dir = ""
    modal = "dir"  # å›ºå®šä½¿ç”¨ dir æ¨¡æ€
    max_workers = 1000
    batch_size = 1000
    
    print(f"ğŸ¯ {modal.upper()} æ¨¡æ€å›¾åƒè´¨é‡è¯„ä¼°å·¥å…·")
    print("=" * 60)
    print(f"ğŸ“ æç¤ºæ–‡ä»¶: {prompt_path}")
    print(f"ğŸ–¼ï¸ ç”Ÿæˆå›¾åƒè·¯å¾„: {gen_base_path}")
    print(f"ğŸ“¤ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ­ æ¨¡æ€ç±»å‹: {modal}")
    print(f"ğŸ”§ æœ€å¤§çº¿ç¨‹æ•°: {max_workers}")
    print(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print()
    
    try:
        # åˆ›å»ºå¹¶è¿è¡Œè¯„ä¼°å™¨
        evaluator = SingleModalEvaluator(
            prompt_path=prompt_path,
            gen_base_path=gen_base_path,
            output_dir=output_dir,
            modal=modal,
            max_workers=max_workers,
            batch_size=batch_size
        )
        
        detailed_results, summary_results = evaluator.run_single_modal_evaluation()
        
        if detailed_results is not None:
            print(f"\nğŸ‰ {modal} æ¨¡æ€è¯„ä¼°æˆåŠŸå®Œæˆï¼")
        else:
            print(f"\nâŒ {modal} æ¨¡æ€è¯„ä¼°å¤±è´¥")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
